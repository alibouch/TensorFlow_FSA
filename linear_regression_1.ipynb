{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alibouch/TensorFlow_FSA/blob/master/linear_regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUHAPV8AkyGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhM8FBptk4jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_examples = 1000 # number of training examples\n",
        "training_steps = 1000 # number of steps we are going to train for\n",
        "display_step = 100 # after multiples of this, we display the loss\n",
        "learning_rate = 0.01 # multiplying factor on gradients\n",
        "m, c = 6, -5 # gradient and y-intercept of our line, edit these for a different linear problem\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqEegDIjk6Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to calculate our predicted y, given weight and bias (m and c):\n",
        "def train_data(n, m, c):\n",
        "    x = tf.random.normal([n]) # n values taken from a normal distribution,\n",
        "    noise = tf.random.normal([n])# n values taken from a normal distribution\n",
        "    y = m*x + c + noise # our scatter plot\n",
        "    return x, y\n",
        "def prediction(x, weight, bias):\n",
        "    return weight*x + bias # our predicted (learned) m and c, expression is like y = m*x + c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K5x-gpGlK2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to take the initial, or predicted, weights and biases and calculate the mean-squared loss (deviation) from y:\n",
        "\n",
        "def loss(x, y, weights, biases): \n",
        "    error = prediction(x, weights, biases) - y # how 'wrong' our predicted (learned) y is\n",
        "    squared_error = tf.square(error)\n",
        "    return tf.reduce_mean(input_tensor=squared_error) # overall mean of squared error, scalar value.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY7rcWFZlPgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is where TensorFlow comes into its own. Using a class called GradientTape(), we can write a function to calculate the derivatives (gradients) of our loss with respect to our weights and bias:\n",
        "\n",
        "def grad(x, y, weights, biases):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_ = loss(x, y, weights, biases)\n",
        "      return tape.gradient(loss_, [weights, biases]) # direction and value of the gradient of our weights and biases\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D80znJvylUh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up our regressor for the training loop and display the initial loss as follows:\n",
        "\n",
        "x, y = train_data(n_examples,m,c) # our training values x and y\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Figure 1: Training Data\")\n",
        "W = tf.Variable(np.random.randn()) # initial, random, value for predicted weight (m)\n",
        "B = tf.Variable(np.random.randn()) # initial, random, value for predicted bias (c)\n",
        "\n",
        "print(\"Initial loss: {:.3f}\".format(loss(x, y, W, B)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFoZlvr-rJpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step in range(training_steps): #iterate for each training step\n",
        "  deltaW, deltaB = grad(x, y, W, B) # direction(sign) and value of the gradients of our loss    # with respect to our weights and bias\n",
        "  change_W = deltaW * learning_rate # adjustment amount for weight\n",
        "  change_B = deltaB * learning_rate # adjustment amount for bias\n",
        "  W.assign_sub(change_W) # subract change_W from W\n",
        "  B.assign_sub(change_B) # subract change_B from B\n",
        "  if step==0 or step % display_step == 0:\n",
        "    # print(deltaW.numpy(), deltaB.numpy()) # uncomment if you want to see the gradients\n",
        "    print(\"Loss at step {:02d}: {:.6f}\".format(step, loss(x, y, W, B)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCKFR4snrNEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The final results are as follows:\n",
        "\n",
        "print(\"Final loss: {:.3f}\".format(loss(x, y, W, B)))\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
        "print(\"Compared with m = {:.3f}, c = {:.3f}\".format(m, c),\" of the original line\")\n",
        "xs = np.linspace(-3, 4, 50)\n",
        "ys = W.numpy()*xs + B.numpy()\n",
        "plt.scatter(x,y)\n",
        "plt.plot(xs,ys, color='red')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Figure 2: Line of Best Fit\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}